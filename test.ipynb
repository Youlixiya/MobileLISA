{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "/home/shangyu/miniconda3/envs/nerf2mesh/lib/python3.9/site-packages/transformers/generation/utils.py:1535: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Q: What is the largest animal?\n",
      "A: The largest animal is the blue whale, which can weigh up to 179,000 kilograms (300,0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "model_path = 'mtgv/MobileLLaMA-1.4B-Chat'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "\n",
    "prompt = 'Q: What is the largest animal?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=32\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "/home/shangyu/miniconda3/envs/nerf2mesh/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/shangyu/miniconda3/envs/nerf2mesh/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ MobileVLM-1.7B: In the scene, there is a wooden table with a vase on top of it, surrounded by a garden setting. The table is placed in a grassy area, and there are potted plants nearby.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scripts.inference import inference_once\n",
    "\n",
    "model_path = \"mtgv/MobileVLM-1.7B\"\n",
    "image_file = \"../gaussian-splatting/data/360_v2/garden/images_4/DSC07956.JPG\"\n",
    "prompt_str = \"Can you tell me what is in the scene?\"\n",
    "# prompt_str = \"Who is the author of this book?\\nAnswer the question using a single word or phrase.\"\n",
    "# (or) What is the title of this book?\n",
    "# (or) Is this book related to Education & Teaching?\n",
    "\n",
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"image_file\": image_file,\n",
    "    \"prompt\": prompt_str,\n",
    "    \"conv_mode\": \"v1\",\n",
    "    \"temperature\": 0, \n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"load_8bit\": False,\n",
    "    \"load_4bit\": False,\n",
    "})()\n",
    "\n",
    "inference_once(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
